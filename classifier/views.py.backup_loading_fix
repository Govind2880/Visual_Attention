from django.shortcuts import render
from django.http import JsonResponse
from rest_framework.decorators import api_view
from rest_framework.response import Response
from .serializers import PredictionRequestSerializer
import numpy as np
import torch

CLASSES = ["Negative", "Positive"]

def safe_load_model():
    """Safely load the model without causing import errors."""
    try:
        from pathlib import Path
        import torch
        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
        
        # Find model path
        model_paths = ["models", "./models"]
        model_path = None
        
        for path in model_paths:
            model_dir = Path(path)
            if model_dir.exists() and (model_dir / "config.json").exists():
                model_path = str(model_dir.resolve())
                break
        
        if model_path is None:
            model_path = "distilbert-base-uncased"
        
        # Load model
        if model_path == "distilbert-base-uncased":
            model = DistilBertForSequenceClassification.from_pretrained(
                model_path, num_labels=2, output_attentions=True
            )
            tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)
        else:
            model = DistilBertForSequenceClassification.from_pretrained(
                model_path, output_attentions=True, local_files_only=True
            )
            tokenizer = DistilBertTokenizerFast.from_pretrained(
                model_path, local_files_only=True
            )
        
        return model, tokenizer
        
    except Exception as e:
        print(f"Error loading model: {e}")
        return None, None

def predict_with_attention_safe(text, model, tokenizer):
    """Safe prediction function."""
    inputs = tokenizer(
        text, return_tensors="pt", truncation=True, max_length=512, padding=True
    )
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    probs = torch.softmax(outputs.logits, dim=1).detach().numpy()
    attentions = outputs.attentions
    
    return probs, attentions, inputs

def show_attention_simple(text, tokenizer, attentions, inputs):
    """Simple attention visualization."""
    try:
        from src.visualize import show_attention
        return show_attention(text, tokenizer, attentions, inputs)
    except:
        # Fallback simple visualization
        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        
        # Simple attention aggregation
        if attentions:
            # Get attention from last layer, average across heads, from CLS token
            last_attention = attentions[-1]  # [batch, heads, seq_len, seq_len]
            cls_attention = last_attention[0, :, 0, :].mean(dim=0)  # Average across heads
            scores = cls_attention.detach().cpu().numpy()
            
            # Normalize scores
            if scores.max() > 0:
                scores = scores / scores.max()
            
            html_parts = []
            for token, score in zip(tokens, scores):
                # Skip special tokens
                if token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
                    continue
                
                # Clean token (remove ## for subwords)
                clean_token = token.replace('##', '')
                if not clean_token.strip():  # Skip empty tokens
                    continue
                
                # Color intensity based on attention score
                alpha = max(0.1, min(0.9, score))  # Ensure visibility
                color = f"rgba(255, 0, 0, {alpha:.2f})"
                
                html_parts.append(
                    f'<span class="attention-word" style="background-color: {color}; '
                    f'padding: 3px 5px; margin: 2px; border-radius: 3px; '
                    f'title="Attention: {score:.3f}">{clean_token}</span>'
                )
            
            return ' '.join(html_parts) if html_parts else f'<span>{text}</span>'
        
        return f'<span>{text}</span>'

# Global model cache
_cached_model = None
_cached_tokenizer = None

def get_cached_model():
    """Get cached model and tokenizer."""
    global _cached_model, _cached_tokenizer
    
    if _cached_model is None:
        _cached_model, _cached_tokenizer = safe_load_model()
    
    return _cached_model, _cached_tokenizer

def home(request):
    """Main web interface for the classifier."""
    if request.method == "POST":
        text = request.POST.get("user_input", "").strip()
        
        if not text:
            return render(request, "index.html", {
                "error": "Please enter some text to classify."
            })
        
        try:
            # Load model
            model, tokenizer = get_cached_model()
            
            if model is None or tokenizer is None:
                return render(request, "index.html", {
                    "text": text,
                    "error": "Model could not be loaded. Please check your model files."
                })
            
            # Get prediction
            probs, attentions, inputs = predict_with_attention_safe(text, model, tokenizer)
            label_idx = np.argmax(probs[0])
            confidence = float(probs[0][label_idx]) * 100  # Convert to percentage
            
            # Generate attention visualization
            attention_html = show_attention_simple(text, tokenizer, attentions, inputs)
            
            # FIXED: Prepare probability data with correct structure
            prob_data = []
            for i, class_name in enumerate(CLASSES):
                prob_data.append({
                    "label": class_name,
                    "probability": float(probs[0][i]) * 100  # Convert to percentage
                })
            
            return render(request, "index.html", {
                "text": text,
                "prediction": CLASSES[label_idx],
                "confidence": confidence,
                "probabilities": prob_data,  # This is now correctly formatted
                "attention_html": attention_html,
                "success": True
            })
            
        except Exception as e:
            print(f"Prediction error: {e}")
            import traceback
            traceback.print_exc()
            return render(request, "index.html", {
                "text": text,
                "error": f"Error processing text: {str(e)}"
            })
    
    return render(request, "index.html")

@api_view(["POST"])
def predict_api(request):
    """REST API endpoint for predictions."""
    serializer = PredictionRequestSerializer(data=request.data)
    
    if not serializer.is_valid():
        return Response(
            {"error": "Invalid input", "details": serializer.errors}, 
            status=400
        )
    
    text = serializer.validated_data["text"].strip()
    
    if not text:
        return Response({"error": "Text cannot be empty"}, status=400)
    
    try:
        # Load model
        model, tokenizer = get_cached_model()
        
        if model is None or tokenizer is None:
            return Response(
                {"error": "Model could not be loaded"}, 
                status=500
            )
        
        # Get prediction
        probs, attentions, inputs = predict_with_attention_safe(text, model, tokenizer)
        label_idx = np.argmax(probs[0])
        confidence = float(probs[0][label_idx])
        
        # Generate attention visualization
        attention_html = show_attention_simple(text, tokenizer, attentions, inputs)
        
        return Response({
            "text": text,
            "prediction": CLASSES[label_idx],
            "confidence": confidence,
            "probabilities": {
                class_name: float(prob) 
                for class_name, prob in zip(CLASSES, probs[0])
            },
            "attention_html": attention_html,
            "success": True
        })
        
    except Exception as e:
        print(f"API prediction error: {e}")
        import traceback
        traceback.print_exc()
        return Response(
            {"error": "Error processing text", "details": str(e)}, 
            status=500
        )

@api_view(["GET"])
def health_check(request):
    """Health check endpoint."""
    try:
        model, tokenizer = get_cached_model()
        model_loaded = model is not None and tokenizer is not None
        
        return Response({
            "status": "healthy" if model_loaded else "unhealthy",
            "model_loaded": model_loaded,
            "message": "Attention-based Text Classifier is running"
        })
    except Exception as e:
        return Response({
            "status": "unhealthy",
            "error": str(e)
        }, status=500)
    
@api_view(["GET"])
def model_loading_progress(request):
    from src.inference import get_loading_progress, is_model_ready
    progress = get_loading_progress()
    return Response({
        "status": progress["status"],
        "progress": progress.get("progress", 0),
        "step": progress.get("step", ""),
        "is_ready": is_model_ready()
    })